[["index.html", "ML Ethics Wiki", " ML Ethics Wiki Kyle Jablon 2021-04-26 Figure 0.1: Credit to Unsplash Welcome to the ML Ethics Wiki! We encourage all readers to open a pull request if they feel that they have something to contribute to the wiki. I’ll be sure to review, and get back to it as soon as possible! Consider this wiki to be a starting point, the work of creating a comprehensive source for machine learning ethics will be herculean. "],["02-intro.html", "Chapter 1 Introduction 1.1 The Problem 1.2 Who is this for? 1.3 Why do ethics matter? 1.4 Why does sociology matter? 1.5 Why does institutionalization matter? 1.6 What is machine learning and why is it relevant? 1.7 Additional Resources", " Chapter 1 Introduction 1.1 The Problem The field of technology is facing an “ethics crisis.” Figure 1.1: An article from the New Yorker Technology does not exist on an island by itself; technology is used by people, and is often used to allocate resources that are inherently finite in nature. Throughout this process, technology becomes political, whether one likes it or not. There are inherently consequences for this type of thinking, and, one cannot just look away. 1.2 Who is this for? The tech ethics crisis is not merely algorithmic, and affects the industry at large. However, almost no problem exists in a vacuum. Issues of diversity in the field do have impacts on issues such as algorithmic discrimination. Issues of privacy also have impacts on dataset bias. As such, this website will function as a self-directed resource for tech ethics in general, with a focus on machine learning and data science ethics in particular. This site hopes to elucidate many (but obviously not all) of the ethical issues practitioners in technology and machine learning face, elucidate how to identify when these situations come up, and offer up appropriate solutions that work at an institutional level. As for the talent, because demand vastly exceeds supply for high quality engineering talent (at least in America). Engineers, or technology workers are empowered in ways that many other workers are not. In addition, employees in the industry care about behaving ethically. A 2017 poll of data science and machine learning practitioners on the industry site, kdnuggets found 76% of respondents agree with the sentiment that “data science should include ethics training” (Blog 2015). Ultimately, the target audience for this wiki includes data engineers, data scientists, machine learning engineers, data analysts, and all people who work in the greater data landscape that must grapple with the ethical issues regarding ethics and machine learning. Figure 1.2: 2017 KDNuggets Poll 1.3 Why do ethics matter? For a Kantian, ethics matters because the the only good in and of itself is a good will. There is no other good in the world, because it can always be corrupted by its context. Bravery may be an admirable trait, but not for an SS officer. Thus, the only good in and itself is a good will. A good will must necessarily be motivated only by reason, it cannot be influenced by anything because the only thing known, a priori (before all knowledge) is that it is rational. Thus, this reason must be the guiding principle of its actions. Thus, the rational being is forced to acknowledge that if it recognizes any sense of good, then it must also recognize reason. In short, ethics matters because human beings know better. On a more practical level, ethics is the name of the game right now at major artificial intelligence computer conferences. According to Matthew Kearns, a professor of Computer Science, “The academic research in the field has been deployed at massive scale on society, […] with that comes this higher responsibility.” (Hutson 2021). Regulation will likely arrive at some point in the future, but in the meantime, the field is governed by norms of acceptable and unacceptable behaviour, and self-policing is the norm. Meanwhile, practitioners themselves are increasingly concerned with issues of ethics, and even if one practitioner may view themselves as “just a researcher,” it is quite likely that their peers will be quite so generous (Hutson 2021). Thus, if one considers oneself part of the AI/ML/tech community, it is inevitable that one is required to concern oneself with ethics. 1.4 Why does sociology matter? If technology is a mere tool, then it is certainly relevant who wields and utilizes the tool. Sociology is “the study of human social relationships and institutions” (Electronic Article, n.d.a). One of the most damaging myths promulgated about technology is that technology exists in a vacuum; this is untrue. The technology built by engineers is a component of other technological and societal systems. A search engine is not just a search engine; if it is adopted, it will be used by police officers, teachers, and students who all come to the product with their own assumptions. 1.5 Why does institutionalization matter? When software engineers build products, they are often asked “but how will it scale?” The dilemma for ethical issues is the same. Even if one individual is faced with an ethical quandary, and they resolve it, oftentimes, it does not solve the issue for the institution. Just as engineering products must scale for data, or number of users; so too must ethical solutions to technical problems must also scale for engineers, and organizations. Thus, this process of “institutionalization” is one that will come up again and again throughout this website. In addition, this also comes with an acknowledgement that readers are just people; there is only so much that one person can do to change an organization (especially at lower levels). However, there is still much that can be done by rank and file employees in order to do change both what they can, and pave a smoother path for others who might come after them. Institutionalization comes into relevance for practitioners, because the strucutre of this wiki intends to encourage practitioners to think structurally. The solutions that are promoted here will include the details that can be implemented on a project by project basis, but will hopefully also include steps that practitioners can take to help shape their organizations into normalizing, or “institutionalizing” ethics as part of the machine learning project process. 1.6 What is machine learning and why is it relevant? As the great Aurélien Géron defines it, “Machine Learning is the science (and art) of programming computers so they can learn from data” (Géron 2019). Practitioners in the field of machine learning are often called machine learning engineers, research engineers, data scientists, research scientists, and occasionally software engineers. Figure 1.3: From Geron’s ’Hands on Machine Learning 1.7 Additional Resources An article to read: “Do Artifacts Have Politics?” by Langdon Winner http://moxi.dk/reference/Winner-Do-Artifacts-Have-Politics-1980.pdf An (fun) story to read that involves ML: “Malak” by Peter Watts https://rifters.com/real/shorts/PeterWatts_Malak.pdf "],["03-dataset-bias.html", "Chapter 2 Ethics: Dataset bias and it’s consequences 2.1 Introduction 2.2 The problem 2.3 How to identify the problem? 2.4 Resources", " Chapter 2 Ethics: Dataset bias and it’s consequences 2.1 Introduction Facial Recognition software has disproportinately higher error rates on both women, and darker skinned individuals. A 2018 study on three commerical grade gender-classifiers found error rates in classifying darker skinned females were up to 42 times greater than error rates in classifying lighter skinned men (Buolamwini and Gebru, n.d.). Across the board classifiers performed better in classifying male faces than female faces, and all classifiers performed better on lighter faces than on darker faces. These classifiers were not developed by unknown companies, they were developed by IBM, Microsoft, and Face++. In other words, these were commercial products that were developed for the sole purpose of gender classification. Even on a more practical level Torralba and Efros (2011) lays out the issue with dataset bias amongst common computer vision datasets. Specifically, by training a model to detect objects amongst multiple common datasets in the field, and then making predictions on other common datasets in the field, one would expect relatively decent performance. In fact, the results show some severe problems. For example, one model tagged an image of a sofa as a “car.” The issue is, again, in effect, dataset bias and a lack of representation from the datasets that are available. 2.2 The problem There are over 23 sorts of bias, many of which will be listed below. Most commonly, the issues appear in two forms: exclusion bias, or measurement bias. Exclusion bias occurs when the sample population does not represent the population distribution as a whole. Perhaps a health insurance company seeks to build a model, and while they are trying to create a model for the entire country, they decide to pull training data from the city of Denver. While Denver is a perfectly fine city, the population distribution of Denver will not reflect the United States as a whole. According to the US Census, Denver is roughly 55% white and 10% black, whereas Colorado as a whole is roughly 68% white and 5% black, and the US as a whole is roughly 60% white and 13% white. Obviously no dataset is going to be perfect, but efforts must be made to rebalance the dataset against the specific sort of imbalance ocurring here. Exclusion bias does not just occur with regards to racial imbalance, it can also occur due to improper balancing of socioeconomic status, age, or gender. In addition, exclusion bias does not merely affect populations; it can also refer to the actual data collected. For example, predictive software has been developed to predict rates of reoffending for prisoners. In turn, this data is then used to determine length of sentence. Individuals with high scores are likely to receive longer sentences. A multitude of data points go into these statistical models, and an unfortunate side effect of much of this research is that it discriminates against it’s users. Individuals with incredibly similar histories, but different racial backgrounds experience vastly different sentences (Julia Angwin and Kirchner 2016). Figure 2.1: Reoffending rates in Florida Measurement bias occurs when the actual measurement of the data collected is biased. In other words, the process of collecting data could be unduly influencing the data being collected. A 2016 by (Hoffman et al. 2016) found that roughly half of medical students in training falsely believed black patients had thicker skin than white patients, and were more tolerant of pain; even when black patients complained about pain, doctors were less likely to note the pain or give them the benefit of the doubt. This is validated by research that shows that hispanic patients are 22% less likely to receive opiates than white patients, and black patients are 22% less likely to receive “any analgesia” for pain (Meghani, Byun, and Gallagher 2012). Consider a model developed in the medical field that gauged “pain claimed by patients” as a feature. Or consider a model that classifies X-Rays and checks for malignant growths; training data comes from a variety of different X-Ray machines, but if one machine generates “stretched” images that while not necessarily faulty are significantly different from the other machines, there would be measurement bias introduced into the dataset. It is important to describe the different sorts of bias, because it elucidates the inherently biased nature of data. Mehrabi et al. (2019) describes the following types of bias: Historical Bias Representation Bias Measurement Bias Evaluation Bias Aggregation Bias Population Bias Simpson’s Paradox - A trend that Longitudnal Data Fallacy Sampling Bias - “a trend, association, or characteristic observed in underlying subgroups may be quite different from association or characteristic observed when these subgroups are aggregated” Behavioral Bias Content Production Bias Linking Bias Temporal Bias Popularity Bias Algorithmic Bias User Interaction Bias Social Bias Emergent Bias Self-selection Bias Omitted Variable Bias Cause-Effect Bias Observer Bias Funding Bias 2.3 How to identify the problem? The first thing that can be done is to recognize that every dataset is biased. Datasets are generated on the actions of human behaviour, and humans have their own preconceived notions, and biases. Absent any filtering, or adjustment done by a machine learning or data science practitioner, these biases will merely get algorithmically reinforced. The old adage that “numbers don’t lie” is perhaps true, but data frequently does. It is up to practitioners to identify manners in which selection bias could be occurring in their dataset, prior to utilizing it. From there the next step is to determine if any reconciliation needs to be made. ## What can be done? Most commonly, dataset bias manifests itself as a form of selection bias in one’s dataset. As such, the solutions to selection bias are often to resample the dataset in order to rectify the bias. Figure 2.2: A comic from Antonio Torralba’s paper In the case of measurement bias, it must be up to the data scientist to assess any possible sources of measurement bias that could be generated in the collection of data. It may be up to a practitioner to either significantly modify a given feature, or reject it entirely if it affects models significantly. A simple solution is to create extreme data and feed it into a model. Even if a practitioner does not have the ability to control the qualify of their training data on a large scale, they do have the ability to create smaller test sets that are more balanced, and then feed those into predictive models as final “sanity checks.” Finally, logging and tracking is an incredibly important mechanism with models that have disparate issues with different subgroups. In many cases, practitioners should be able to assess if a model does poorly with certain groups of people prior to launching the model in production. Noting a models deficiencies to stakeholders is a non-negotiable aspect of doing one’s jobs, and that extends to issues of equity. 2.4 Resources An article to read: “Garbage in, Garbage Out” by Claire Garvey https://www.flawedfacedata.com/# A story to read: “Unbiased look at dataset bias” by Antonio Torralba https://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf "],["04-privacy-rights.html", "Chapter 3 Ethics: Privacy Rights 3.1 What do Privacy Rights have to do with tech? 3.2 What are privacy rights? 3.3 Rules of thumb 3.4 Resources", " Chapter 3 Ethics: Privacy Rights 3.1 What do Privacy Rights have to do with tech? The relationship between privacy rights and technology has become distorted in the past series of decades. In particular, the rise of advanced machine learning algorithms and the mass collection of data have created a series of ethical more complicated than those in George Orwell’s 1984. Consider the following example, in the 1990’s CCTV was standard in retail stores across America. Suppose McDonald’s for instance chose to take all CCTV footage from all stores across the United States and collect said CCTV footage in a single database and then apply image recognition software to automatically flag incidences of stealing or vandalism in stores and then call police automatically whenever such incidents occurred. To the knowledge of this author, McDonald’s, nor any other major retail chain has made any attempt to engage in the prior action. However, it is indisputable that such a surveillence system, as described before could theoretically be built. Facebook regularly handles petabytes of data, and also applies image recognition software onto it’s own stockpile of images. Similarly, consider the Cambridge Analytica scandal. Cambridge Analytica harvested Facebook user data, and claimed to create targeted “user profiles” that were of such levels of specificty that actors could inject hyper specific content into users newsfeeds to shape their reality. This is all before delving into security issues. If a health insurance provider collects a social security number, and then gets hacked then all users social security numbers could be considered vulnerable. The scale of security breaches appears to be magnitudes larger than what was possible decades prior. The issue is the same regardless, the scale of data collection and amplification of computing power has made possible techniques that were improbable decades ago, bringing new rise to data privacy issues. 3.2 What are privacy rights? The standard model of privacy regulation encompasses the “notice and consent” model, wherein individuals are notified that their data is being collected, and that they must consent to the collection of their data (Kerry 2020). This is most evident in the barrage of user agreements that individuals must deal with in signing up for any service. However, this model simply doesn’t scale well to deal with newer data issues, wherein data is being collected by user browsing, or by public cameras that don’t possibly have any opportunity to inform users that their data is being collected. 3.3 Rules of thumb Collect only the data necessary. While it is certainly nice to maximize the amount of data collected, it is not without risk. Collecting unnecessary data opens up a company up to additional security risks, and can also create scaling issues for a data infrastructure. Most importantly though, it’s simply unnecessary, and opens the door to potential legal liability. Tell users what data is being collected, and what it’s being used for. Encrypt personally identifiable information. Plain text versions of social security numbers or passwords should ever be stored. There are a variety of encryption schemes, but the most standard involves salting a password, and then hashing the result. Apathy is not an excuse. Many practitioners defend the ethically questionable nature of the software they create by positing that they are “just an engineer” or “just a researcher.” This is simply untrue. The notion that someone could cordone off their impact to society and the world to one minor slice of their identity is utterly fanciful. Rightly or wrongly, all are judged for their contributions to society. Those who are “just researchers” ought to take extra steps to ensure that their research is not used in a manner that is unethical. Surveillance software should be viewed with high suspicion. Vision software that claims to “monitor” individuals for proper/improper behaviour is always highly suspect for additional scrutiny. 3.4 Resources An film to watch about the risks of being data-driven: “The Fog of War” by Errol Morris https://www.amazon.com/Fog-War-Robert-McNamara/dp/B001EJGQTU A story to read: “The Insecurity Machine” by Astra Taylor https://logicmag.io/security/the-insecurity-machine/ "],["05-environment.html", "Chapter 4 Ethics: Environmental Costs of Machine Learning 4.1 The Problem 4.2 How to identify the problem 4.3 What can be done 4.4 Resources", " Chapter 4 Ethics: Environmental Costs of Machine Learning 4.1 The Problem Recent machine learning models, such as BERT and XLNet have received state of the art results on common natural language processing tasks. However, not often mentioned is the sheer cost required to train these models. They often cannot be trained on a single computer, and typically require multiple GPU’s in the cloud. For instance, Google’s BERT-large model contains 350 million parameters, and openAI’s GPT-2 contains 1.5 billion parameters leading to massive costs. For instance, XLNet was trained on 512 TPU chips for 2.5 days (Schwartz et al. 2019). As models have gotten more, and more complex, training time can even extend into weeks and months for models that are even more complicated. Recent research has shown that that “training BERT on GPU is roughly equivalent to a trans-American flight” (Strubell, Ganesh, and McCallum 2019). Most severely, the NAS language model, has a carbon output roughly equivalent to the cumulative output of 5 cars over their lifetimes. Because scientists estimate that carbon emissions must be cut in half by the next decade, this is an “all hands on deck” situation for humanity. The training of Deep Learning models can be quite significant in scope, and also the training of more and more powerful models become more normalized. Thus, rising energy usage in the Deep Learning field, in conjunction with a need to cut carbon output worldwide will lead to increased attention of carbon outputs in the Deep Learning field. 4.2 How to identify the problem Identifying this problem is straightforward. Most machine learning practitioners are not training Deep Learning language models for months at a time. In addition, training machine learning models on local machines is likely fine. However, those who are training large machine learning models for days at a time will likely encounter questions regarding the carbon impact of their model. Specifically, this issue pops up at scale when one is training a model across heavy hardware, usually on the cloud. Practitioners ought to calculate the carbon costs of their model if this is the case to gauge an estimate of whether or not their model is large enough to trigger carbon offset concerns. 4.3 What can be done All of these ideas are taken from Lacoste et al. (2019), and the fantastic job they’ve done in outlining what to do to make machine learning more environmentally friendly. Fortunately, there is also quite a lot that can be done to cut down on one’s carbon impact! Start by measuring the carbon impact of the larger models that you’ve trained and encouraging others to do so. Generally, when a model is trained and productionized for an organization, the creator is responsible for creating documentation covering the models. This is the perfect time to list the carbon impact of your model. Encourage others to measure the carbon impact of their models, the cost is actually quite small and the benefit of such a norm is extremely positive. If you have a choice, verify that your cloud provider is carbon neutral. Microsoft Azure, and Google Cloud are both carbon neutral, and AWS is on the way to becoming carbon neutral (Lacoste et al. 2019). As of 2016, 44% of Microsoft Azure’s energy usage came from renewable sources, and as of 2017, 56% of Google’s cloud energy usage came from renewable sources (Strubell, Ganesh, and McCallum 2019). Another item worth looking at is the Power Usage Effectiveness (the percentage of energy not used on the computing, cooling or other required auxiliary functions themselves). Consider changing your data center location. While a cloud provider may be carbon neutral, a particular cloud center might not be due to the grid that that cloud center is connected to. Typically, data centers in North America or Europe are more likely to be cloud neutral than others. The most efficient centers may be up to 40 times more efficient than the least efficient cloud centers (Lacoste et al. 2019). Figure 4.1: Carbon emissions from data centers around the world Be efficient with your code, and hardware! Avoid brute force grid searches for hyperparameter tuning; research shows that random grid search is a more effective replacement. Implement early stopping for any hyperparameter tuning when possible such that the full time required to train a model is not required. Also, be careful with selected hardware. Research shows that GPU usage can be 10 times more efficient than CPU’s, and TPU’s can in turn be 3 to 4 times more efficient than GPU’s. Making sure to choose the right hardware will speed up model training, and in turn reduce the overall amount of carbon locations. This should go without saying, but don’t train more complicated models than are required! Deep learning is expensive, and if a simpler tool can be used to accomplish the same goal, then it’s likely that that is the way to go! 4.4 Resources Measure your carbon impact: https://mlco2.github.io/impact/ "],["06-black-box.html", "Chapter 5 Ethics: Black Box Models 5.1 The Problem 5.2 Which models are black box models 5.3 What can be done 5.4 Resources", " Chapter 5 Ethics: Black Box Models 5.1 The Problem From Philip K Dick’s science fiction classic “Minority Report” describing the notion of “precrime”: “You’re acquainted with the theory of precrime, of course. I presume we can take that for granted.” “I have the information publicly available,” Witwer replied. “With the aid of your precog mutants, you’ve boldly and successfully abolished the post-crime punitive system of jails and fines. [...] Anderton said: “You’ve probably already grasped the basic legalistic drawback to precrime methodology. We’re taking in individuals who have broken no law.” “But surely, they will,” Witwer affirmed with conviction. “Happily, they don’t — because we get to them first, before they can commit an act of violence. So the commission of the crime itself is absolute metaphysics. We can claim they are culpable. They, on the other hand, can eternally claim they’re innocent. And, in a sense, they are innocent.” The notion of “precrime” isn’t merely science fiction. In Philip K. Dick’s story, “precog” mutants who can see the future are used to predict future crimes and then charge and jail prospective offenders, despite the fact that they have not committed any crimes. In the real world, there are parallels that already exist. The COMPAS system is used to predict rates of re offending by prospective criminals, and adjust their sentences accordingly. More importantly, this software is proprietary, and uses up to 130 factors, “including criminal history, age, gender and other information, such as whether their mother was ever arrested or whether they have trouble paying bills” (Rudin 2018). COMPAS has made mistakes in the past, for example, in 2016, an inmate was mistakenly denied parole because an employee mistakenly filled an incorrect answer on a form (Rudin 2018). These risk scores are used for sentencing requirements, and have been shown to disproportionately rate black candidates as being “higher risk” (Julia Angwin and Kirchner 2016). These are all examples of “black box models.” Black box models refer primarily to those models relationship between inputs and generated outputs are not human interpretable. Black box models do not merely refer to machine learning models, although that is primarily where they come up; a financial model can be a black box model if the relationship between inputs and outputs is inscrutable to common users. In other words, a model such as COMPAS is a black box model, in so far as its logic and mechanisms are a invisible to the user, and cannot be deciphered. There is merely the data, the model, and an output which is spit out. Black box models have particular issues in the sense that they can easily exacerbate existing issues of inequality, and make it difficult to carve out exceptions to cases that ought not be treated in a simple way. In particular, black box models can be particularly problematic in exacerbating underlying racial or socioeconomic relationships in data. For example, a black box model that assigns insurance premiums can “learn” notions of race from the underlying characteristics of data, such as perhaps zip code or other socioeconomic information and then amplify existing underlying biases. The nature of these models as black boxes means that they can essentially automate pre-existing inequality with little to no accountability for the problematic nature of the model. 5.2 Which models are black box models Models can become “black boxes” for a few primary reasons: typically this can be because a model is proprietary, user impenetrable, or their structure is quite literally not human scrutable. For example, COMPAS, is a proprietary model, an interpretable model that does not explain its predictions to its users is still a black box, and finally a model where no one understands how it works would not be human scrutable. 5.3 What can be done It should be noted that there are black box models, and black box systems. Not every model necessarily needs to be an open book. Google Translate is an incredibly complicated neural network model, and while it would be nice for it to be human scrutable, but it is likely unnecessary for this to be totally human interpretable. Meanwhile, an algorithm that prices insurance rates for different people should be explainable, and a black box system creates an opportunity for discrimination. This distinction is important, because merely constructing an explainable model does not necessarily lead to explainable systems. In other words, having an interpretable model is a necessary but insufficient standard for an interpretable machine learning system. The best solution of all is a simple model, and regression models are typically the most straight forward of all. If a simple model can get the job done, it goes without saying that that should always be the first option. Regression models have extensive explainability tools built around them, such as LIME or SHAP, and are also easiest to build more tooling on top of, due to their widespread use and availability. If the typical regression models (OLS and logistic) cannot do the trick it has been suggested that the most powerful and explainable model possible is an xgboost model when used in conjunction with shapley (Film or Broadcast 2018). In particular, the ability to manually set monotonicity of the input features results in incredibly interpretable models. For example, if one is predicting health insurance premiums, age can be monotonically related to one’s premiums and thus as age increases then one’s premiums will naturally go up. In addition to the SHAP framework, which is a game theoretic approach to explainable ML, this is an incredibly powerful tool. Another mechanism if a model is particularly complicated, then a user can train a complicated model and then train an explainable model to predict the output of the complicated model. This solution typically results in a moderate degrade in accuracy, but is nonetheless a very popular workaround to convert a black box model to an explainable one. Mitchell et al. (n.d.) has suggested standardizing model cards for educating cross-functional stakeholders on machine learning. Akin to a nutritional scorecard, but for a machine learning model, this is a model intended to standardize information about a machine learning model on a one-pager. This is an important step in standardizing explainable modeling. A model card would include information on training data, and the level of explainability provided by the model and system as a whole, in addition to any possible gaps in its coverage and possible biases. The mere act of standardizing these at an organization can go a long way towards norm creation in the indsutry as a whole. 5.4 Resources Interpretable ML Book by Christoph Molnar: https://christophm.github.io/interpretable-ml-book/ The Minority Report by Philip K. Dick: https://cwanderson.org/wp-content/uploads/2011/11/Philip-K-Dick-The-Minority-Report.pdf "],["07-amplification.html", "Chapter 6 Ethics: Amplification 6.1 The Problem 6.2 What can be done 6.3 Resources", " Chapter 6 Ethics: Amplification 6.1 The Problem 6.1.1 Algorithmic Determinism The Alleghany County Office of Children Youth and Families runs a system called the Key Information and Demographic System, or KIDS; effectively, this is the part of the local government that deals with issues of child neglect and abuse, and the county uses a risk model called the Allegheny Family Screening Tool (AFST) to forecast child abuse and neglect. As Eubanks (2018) points out, this system is composed of a model that is at it’s core a regression. Ideally, ASFT would predict child maltreatment, but for obvious reasons, measurement is impossible; instead, the system predicts the proxy variables of “community re-referral” where multiple calls are received for the same child within two years, and the proxy variable of “child placement,” when a call results in a child being placed in foster care within two years. According to Eubanks (2018), “the ASFT actually predicts decisions made by the community (which families will be reported to the hotline) and by the agency and the family courts (which children will be removed from their families).” Because three quarter of cases are actually “child neglect” cases, and the line for neglect can be quite vague, poor families disproportionately get reported for child neglect. This most commonly occurs when parents leave their children at home while they go to work. Once a child gets placed in the system, they are already flagged once, and their risk assessment scores increase. Poor and minority families are more likely to get reported on face. Subsequent referrals thus create a reinforcing cycle, once a report has been made, future risk assessments are more likely to be triggered as problematic risk assessment. This is a case of an algorithmic system constructing a feedback loop, thereby amplifying pre-existing bias that already exists. It has also been called “algorithmic determinism.” However, it is not the only sort of bias that exists. 6.1.2 Amplifying Misinformation In 2018, a study by Vosoughi, Roy, and Aral (2018), titled “The Spread of True and False News Online” which found that false content was 70% more likely to be shared than true content, and that it also takes true stories about six times longer to reach users as for false stories to reach the same number of people. Users receive their news from algorithmically driven social media; these social media organizations optimize for engagement, and at the end of the day false content drives a sense of “novelty” and in turn, greater engagement. Building on this, Phillips (2018) goes one step farther and lays out how the alt-right manipualted journalists, and recommendation systems in order to amplify their message. A small but very vocal group of bad faith actors would begin by manufacturing outrage, and bombarding their victims with abuse on social media. By generating enough outrage, the content would go viral, at which point, mainstream media outlets would cover the the material. Because journalists are inclined to cover both sides of the issue, they cover both conflicting sides. Thus, this mainstream media coverage would result in more users sympathizing with bad faith actors, generating even further mainstream media coverage and amplifying the message even more prominently in society. Consider an example from Boyd (2017), wherein computer scientist Latanya Sweeney, searched her own name on google and found a series of advertisements “inviting her to ask if she had a criminal record.” In turn, she ran a series of black and white names through the search engine and found only the black names generated criminal justice results. This information in turn suggested that the search engine had in effect “learned” about racial biases from user search history. In the former case, a series of bad faith actors had gamed a series of recommendation engines in order to amplify their message, whereas, in the latter case a search engine learned underlying human biases and amplified the results right back. What is worth noting though is that this amplification loop is not unique to social media, in essence, it applies to all algorithmically generated content, and recommendation systems. Any recommender system that uses mass user-generated data is vulnerable to this sort of amplification. Even if one does their best to try to sanitize a dataset that may contain racialized content or bias, at the end of the day, “no amount of excluding certain subreddits, removing of categories of tweets, or ignoring content with problematic words will prepare you for those who are hellbent on messing with you” (Boyd 2017). 6.2 What can be done Unfortunately, amplification is a feature not a bug of large technical systems. Large technical systems are designed to scale, and many machine learning systems are created specifically to enable this scaling. Most systems can eventually be “hacked” in order to amplify problematic content or results, there are even bad actors that bank on it. That being said, documentation, an open door policy, stress testing and early detection are key. Most importantly, however, what is needed is a mindset from practitioners to consider and think about manners in which their machine learning systems can be ‘hacked’ to amplify the aims of bad actors. Race and socioeconomic should be warily used as inputs for algorithmic models, as should data that functions as a proxy for race and socioeconomic status (zip code, high school). These inputs have the potential to amplify existing inequality trends, and create deterministic output based on the inputs. Brainstorming sources of bias is a helpful intellectual exercise to explore the possibilities of misuse of any AI system. This can be done with a handful of practitioners where they brainstorm and note possible sources of bias that could be both introduced and amplified in their machine learning system. Once the brainstorming is completed, noting possible results in any model documentation is integral to see if these issues crop up again. In a more structured manner, Mitchell et al. (n.d.) has suggested utilizing model cards for all productionized machine learning models. In addition, impact analyses are a helpful intellectual exercise to brainstorm manners in which underlying systems can be possibly “broken.” Acceptance testing is integral in any technical product being built. For machine learning models in particular, creating extreme sample data in order to run “sanity tests” on the resulting machine learning model being implemented is integral. This sample data can be derived from existing sample data, but with artificial changes in specific columns relating to race, or geolocation. This provides an straightforward mechanism to stress test an algorithmic system for possible amplification bias. 6.3 Resources An article to read: “Alternative Influence” by Rebecca Lewis https://datasociety.net/wp-content/uploads/2018/09/DS_Alternative_Influence.pdf "],["08-deep-fakes.html", "Chapter 7 Ethics: Deep Fakes 7.1 Introduction 7.2 What can be done", " Chapter 7 Ethics: Deep Fakes 7.1 Introduction Deep Fakes are worth their own (minor) module, because the problems posed by them are very specific, and the problems posed by them are all very new in nature. This module will not go into the technology, and how it works, because the ethical issues exist regardless of how the technology works. A deep fake is essentially “computer-manipulated images [or video] in which one person’s likeness has been used to replace that of another” (Kelion 2020). The core ethical issue is that “they mimic a person’s likeness without their permission,” but their mass destructive content comes from their ability to easily spread misinformation (Goodwin 2020). Below is a deepfake example from Strickland (2019). Figure 7.1: A simple deepfake example The history of deep fakes is interesting. For some highlights, however, the first recorded use of the term began in 2017 when a reddit user began creating pornographic content (Goodwin 2020). For one, their use started with pornography. A recent study estimated that roughly 96% of all deep fakes content is pornographic (Agarwal 2020). Notably in 2019, an app called “DeepNude” enabled users to create their own pornographic deep fakes. In 2020, President Trump amplified through his twitter account, a deep fake of Joe Biden (Goodwin 2020). Deep fakes, as previously discussed, are already under the purview of the national government: the 2019 National Defense Authorization Act set up standards for Congress and the Pentagon to address the creation of Deep Fakes by foreign governments and set up funding for “Deep Fake competitions” (Web Page 2019). 7.2 What can be done To be honest, very little can be done about deep fakes right now by individual actors. The technology exists, and while it’s limited, it’s already out there. The founder of Ctrl Shift Face, a popular deep fake YouTube channel has remarked that “If there ever will be a harmful deepfake, Facebook is the place where it will spread […]In that case, what’s the bigger issue? The medium or the platform?” (Agarwal 2020). In such a case, it is really up to engineers and technology companies to recognize if there is a risk for deep fake abuse on their platform. Most companies will likely not have to deal with this, but any company that involves video as a key part of their business model is going to face deep fake issues at some point in the future. There appears to be promising technology to detect deep fakes. In particular, Microsoft, has developed a set of tools to detect deep fakes (Kelion 2020), suggesting that one promising approach is to train neural networks to develop deep fakes. In addition, while there appears to be a growing desire to “legislate” deep fakes, the cat is out of the bag and the technology already exists. What will likely be possible is imposing strict penalties from those who have abuse deep fakes to either manipulate others with misinformation. Another recently proposed solution is to essentially “watermark” all deep fake videos (Chivers 2019). By watermarking all videos, essentially the source of the video could be traced back, and discovered by any investigators dealing with the video. Education, and awareness are necessary. While media literacy is not a panacaea solution, greater awareness of deep fakes and how to spot them is necessary for when they do appear in the future. "],["09-solutionism.html", "Chapter 8 Sociology: Technological Solutionism 8.1 What is technological solutionism? 8.2 The problem 8.3 What can be done? 8.4 Resources", " Chapter 8 Sociology: Technological Solutionism 8.1 What is technological solutionism? Since 2018, Microsoft has been marketing their cloud and AI platforms using a case study for snow leopard preservation (Ho 2018). In an article titled, “How snow leopard selfies and AI can help save the species from extinction,” they detail a project undertaken utilizing their tools wherein an AI algorithm was trained to identify snow leopards utilizing thousands of cameras placed in the wilderness throughout the Himalayas. The hope is that in the future, this algorithm will allow for better estimation, and tracking of the number of snow leopards out in the wild. As evidenced by Microsoft’s marketing, there is a segment of technologists that believe this represents the start of a possible solution to helping save snow leopards. While this certainly could be the case, it is by all means, incredibly unlikely. A quick google search into causes of snow leopard endangerment reveals that snow leopards are endangered for two reasons: first, because they are highly valued in Chinese medicine and average yearly income in nearby areas is roughly $300, and second, due to overgrazing from domesticated animals destroying their natural habitats (Web Page 2008). The long and short of it is that no AI algorithm will address any of these underlying issues. No algorithm can solve for the income inequality causing humans to poach snow leopards, and no algorithm can help repair the snow leopard environment. If anything, the technological solution is merely obfuscating the core of the real issues. The phenomenon of identifying a social, or political problem and addressing it with a technological solution is called “technological solutionism.” Morozov (2013) defines \"technological solutionism, as, an endemic ideology that recasts complex social phenomena like politics, public health, education, and law enforcement as “neatly defined problems with definite, computable solutions or as transparent and self-evident processes that can be easily optimized—if only the right algorithms are in place!” Figure 8.1: A technological solutionist approach to bodegas 8.2 The problem In the words of Latonero (2019): “The deeper issue is that no massive social problem can be reduced to the solution offered by the smartest corporate technologists partnering with the most venerable international organizations.” People often ask, “well what’s the worst that can happen with new technology,” and while there is some validity to this view the issue is related to how new technology is discussed or talked about. There is a manner in which new technology is talked about that can obfuscate the core social or political issue at play. 8.3 What can be done? There are no easy technical answers here. It often makes technical stakeholders uneasy to recognize their own powerlessness to “solve” their way out of their problems. This is natural. The key is to recognize when social or political problems are attempted to be solved by using technology. This is easier than it seems; if an entrepreneur is claiming that their technology is solving a big problem, chances are it’s just not possible. The duty of a technologist in this case should be to appropriate frame teh manner in which their technology or product is being described. There is nothing wrong with “hyping” up a product, so long as the underlying social or political issue is not being hidden. It is hard to create a systemic change to the issue here, and it’s hard to “institutionalize” that technology cannot solve problems. One thing that practitioners can do though is be upfront and clear with the limits of their ability, and point out when technological solutions are overreaching. Promulgating this approach, and critiquing those who engage in “technological solution” is one way to take steps towards creating a better world. 8.4 Resources A talk by Evgeny Morozov on technological solutionism: https://www.youtube.com/watch?v=9yQqrZUD6Gk "],["10-myths.html", "Chapter 9 Sociology: Myths about Silicon Valley and its Workers 9.1 Introduction 9.2 Diversity in Tech is a “pipeline problem” 9.3 Systems need to be maintained 9.4 Resources", " Chapter 9 Sociology: Myths about Silicon Valley and its Workers 9.1 Introduction This module will be a little bit unique in that there is no big overarching “topic” to discuss with a problem and solutions. Rather, we will investigate myths about Silicon Valley and tech workers. In actuality, there are many, but only two will be covered in this module. In so far as technological products are products of their environments, it is worth investigating the environment that many products are developed in and debunking common myths. It is tough to dispute that the ethos and legacy of Silicon Valley has permeated the technology sector at large; the most famous technologists, Elon Musk, Bill Gates, and Steve Jobs are all strongly tied to the myth of the Valley. Even those companies not based in Silicon Valley are undeniably influenced by it, as such, the myths about it have permeated through the country at large. They are worth investigating, and further researching. 9.2 Diversity in Tech is a “pipeline problem” Technology has a diversity and inclusion issues. Consider Google, in it’s 2020 annual diversity report, 5.5% of employees identify as black, 6.6% identify as Latinx, and 32.5% identify as women (Chakravorti 2020). Essentially the myth goes as such, companies would love to hire more diverse candidates, but when they interview candidates, the ones that pass the interview process are disproportionately less diverse. Companies would love a more diverse environment, but they would never want to lower the bar, and the problem is in finding enough qualified candidates. If there were more qualified candidates that were graduating from top colleges, or in entry level positions then these same companies would do a better job hiring a more diverse candidate slate as a result. In 2016, Facebook’s head of diversity, Maxine Williams claimed in defense of their diversity statistics that “it has become clear that at the most fundamental level, appropriate representation in technology or any other industry will depend upon more people having the opportunity to gain necessary skills through the public education system” (Blog 2016), essentially repeating a form of the “pipeline” problem argument This line of thinking has been heavily contested. One way in which this is untrue is that it in essence relies on a form of “credentialing.” For example, if a job requires a PhD or a Masters, then companies will recruit from the best Masters and PhD programs. However, these same programs are also biased against diverse candidates in their own manner. For starters, these programs are incredibly expensive, seemingly ruling out those who do not already have the means to afford them. In addition, these programs also require standardized testing, an indicator which is also highly correlated with wealth and social status. As described by Dr. Joy Rankin, “literally decades of research have shown SATs correlate in no way with how you’re going to do in college or how you might be as a student, but correlate everything with how wealthy your family is, which also then correlates with race and access to all other sorts of things like tutoring and etc. But that same time of credentialing pops up time and time again” (Dickey 2021). In other words, one is not necessarily hiring “better” candidates, but merely those candidates whose credentials most resemble one’s own, simply reinforcing bias back into the structure at paly. 9.3 Systems need to be maintained In 2020 during the COVID-19 pandemic, the state of New Jersey was experiencing huge levels of distress to its state unemployment program. Those who were recently laid off couldn’t sign themselves up for unemployment, and those who could sign themselves up were experiencing massive slowdowns. The governor of the state, Phil Murphy, put out a plea for COBOL programmers. Governor Murphy pinned the issue down by pointing out that “[l]iterally, we have systems that are 40 years-plus old” (Kelly 2020). This problem was not just limited to New Jersey, and affected upwards of a dozen different states who also had unemployment programs built in COBOL (Hicks 2020). But COBOL had been made a convenient scapegoat. COBOL was released in 1960. As described by Jean Sammet, the main architect of COBOL “was certainly intended (and expected) that the language could be used by novice programmers and read by management.” Compared to existing languages, such as FORTRAN, the intention was to “avoid idosyncratic abbrevations, and mathematical symbols that could be difficult to understand.” In fact, one COBOL programmer claimed, “You write COBOL like a novel! Anyone could follow your code.” (Hicks 2020). By 1970, it became the most popular computing language in the world. It is still taught in community colleges around the country, and there are active COBOL communities around the world today. In the words of Mar Hicks, \" the majority of people in the COBOL programmers’ Facebook group are twenty-five to thirty-five-years-old, and the number of people being trained to program and maintain COBOL systems globally is only growing. Many people who work with COBOL graduated in the 1990s or 2000s and have spent most of their twenty-first century careers maintaining and programming COBOL systems.\" However, this same accessibility has been denigrated and criticized. Ironically, the accessibility of COBOL has led to the language being the butt of many jokes. It is frequently called an “old, dead language.” Ironically enough, C, was released in 1972 (roughly 12 years after COBOL), and is not mocked for being called an “old, dead language.” Many trace the insults back to a form of gatekeeping, or “if your code is easy to understand, maybe you and your skills aren’t all that unique or valuable. If management thinks the tools you use and the code you write could be easily learned by anyone, you are eminently replaceable.” (Hicks 2020). This comes into play when it comes to COBOL, because “the narrative that COBOL was to blame for recent failures undoes itself: scapegoating COBOL can’t get far when the code is in fact meant to be easy to read and maintain.” (Hicks 2020). No one is claiming that COBOL is perfect, but the fact is that any well-defined system needs resources, and investment to be maintained over time. Given enough time, even the sturdiest buildings need to be renovated from time to time so that they don’t fall into disrepair. The analogy with code is similar; it’s quite easy to scapegoat a language, but it’s important for engineers to understand that no product can simply be thrown out there and self-maintained. Rather, resources and often money need to be invested in order to maintain systems in the long run. The best engineers understand this, but in the field of government, non-technical stakeholders too often want to “build and ship it,” without thinking about the long-term investments that are required of code just like with physical engineering feats. 9.4 Resources An article to read: https://logicmag.io/care/built-to-last/ "],["11-gdpr.html", "Chapter 10 Law: GDPR 10.1 What is GDPR? 10.2 What are the current problems with GDPR? 10.3 How is GDPR relevant to regulation around the world? 10.4 Resources", " Chapter 10 Law: GDPR 10.1 What is GDPR? When it comes to AI legislation, GDPR is the elephant in the room. It has been so significant that it has been led to Europe being called the “World’s Leading Tech Watchdog” (Newspaper Article 2018). The legislation creates tough privacy regulation, and establishes privacy rights for all European citizens, and all companies dealing with European citizens, even if those companies are not located in Europe. With regards to AI, certain parts of the regulation require that certain algorithmic decision making must be both reviewed and explainable by humans (Li, Yu, and He 2019). Another part of GDPR (Article 17) codifies the “right to erasure” by consumers, posing possible AI compliance issues, with regards to existing data that has already been trained on user data (Li, Yu, and He 2019). International firms are forced to comply with GDPR in order to get access to their market, most notably, Huawei has appointed data compliance officers to deal with the issue (Li, Yu, and He 2019). Other companies have left Europe entirely, YouTube “stopped supporting third-party advertising services on reserved ad buys” after the implementation of GDPR . Yeelight, a smart lighting device company left the market as a result of the regulation. Facebook, and Instagram were immediately sued after their implementation of their policy of “forced consent” of user agreements, and the case is currently pending. Governments were given broad latitude to impose fines of up to 4% of global revenue, with penalties for companies that refused to pay risking fines upward of $1 billion (Newspaper Article 2018). As of 2021, the end result, however, has been called inadequate, critics have complained that enforcement has been adequate, with only Google being fined once for $54 million, with budget deficits for enforcement agencies blamed for the result (Newspaper Article 2020). 10.2 What are the current problems with GDPR? Enforcement has been a persistent issue with GDPR. From May 2018 to April 2020, only a single tech giant was fined: Google was fined 50 million Euros, or “or about one-tenth of what Google generates in sales each day” (Newspaper Article 2020). Other large tech companies, such as Grindr, have been fined in 2021, for violations which the the agency claimed has resulted in “people have had their personal data shared unlawfully” (Lomas 2021). However, while GDPR has the potential to enable countries to level steep fines against large technology companies for violating issues of data privacy, that does not equate with actual enforcement. In the words of a campaigner for privacy regulation, “if you don’t have strong, robust enforcement and investment, this law is a fantasy” (Newspaper Article 2020). GDPR merely gives member states of the European Union the latitude to punish companies that violate GDPR, but the law does not actually force those same countries to punish said companies. Each nation has it’s own data protection agency tasked with enforcing the regulation. In fact, Johnny Ryan, a campaigner for privacy regulation “found that all but three — Germany, Britain and Italy — had data protection agencies with annual budgets of less than €25 million” (Newspaper Article 2020). In addition, Mr. Ryan “found that most countries had only a handful of investigators with industrial expertise dedicated to reviewing technology industry cases.” Apple, Linkedin, Facebook, Google and Twitter are centered in Ireland, and as such the country has taken a center place in discussions over data privacy. Consider that from the time of GDPR’s passage to April 2020, the country had not levvied a single fine. Not until May of 2020 did the country levy a single fine, and even then the first fine was to “Tusla Child and Family Agency” (Web Page 2021a). As of April 2021, Ireland has only levvied a single fine against a major tech company: a 450000 euro fine to Twitter in December of 2020 for “Insufficient fulfilment of data breach notification obligations” (Web Page 2021a). In 2020, 140 people worked at Ireland’s data protection agency, and received a budget of 16.9 million pounds (Newspaper Article 2020). Figure 10.1: All GDPR fines in Ireland up to April 19 2021 Fortunately, not all hope is lost. The law is new, and enforcement takes time. In fact, enforcement claims shot up sharply in 2020. In fact, as of February 2021, 471 instances of GDPR fines were levied, approximately 318 of which were levied in 2020 (Nick Palmieri 2021). This is not to say that enforcement is here to stay, the numbers are still relatively paltry in the grand scheme of the tech sector. However, there does appear to be a gradual improvement on the way. 10.3 How is GDPR relevant to regulation around the world? One unique aspect of GDPR is that regulation applies to all companies that deal with European citizens, regardless of whether those companies are based in Europe themselves. As such, the potential reach for GDPR is huge. GDPR functions as a model for California’s Consumer Protection Act (CCPA), and Virginia’s Consumer Data Privacy Act (VCDPA). Future regulation will likely be modeled on GDPR, depending on its successes or failures. In addition, the reach of GDPR regulation is absolutely massive. Any, and all international companies operating in Europe are forced to deal with GDPR. 10.4 Resources Enforcement Tracker: https://www.enforcementtracker.com/ "],["12-us.html", "Chapter 11 Law: Regulation in the United States 11.1 Introduction 11.2 The National Discussion 11.3 California 11.4 Virginia 11.5 Illinois 11.6 Other States 11.7 Conclusion 11.8 Resources", " Chapter 11 Law: Regulation in the United States 11.1 Introduction There is no official legal legislative document passed by the federal government that covers data privacy or artificial intelligence regulation. Because the focus of this wiki is on “machine learning,” and “data privacy,” we will not discuss general legal regulation, such as Section 230. That being said, there are a patchwork of different policies enforced and handled by different federal agencies. These policies are based more on precedent, and can be changed at any time. In addition, on the state level there is in fact a wide variety of regulations that differ significantly on the state level. Simply attempting to cover the regulation of all states would be beyond the scope of this wiki, and be a major commitment. On the state level, only two states have managed to successfully pass data privacy regulation: California, and Virginia; meanwhile, we will cover an example of another state that has passed some incremental data regulation: Illinois. 11.2 The National Discussion Policy regulation in the United States has been a mixed bag; there has been no national legislation that has been passed regulating AI, or it’s uses, in the vein of GDPR in the EU. However, on the state level, there has been significant legislation passed, regulating the use of artificial intelligence by establishing privacy rights for consumers. The discussion in the United States is still vibrant, and it is not unreasonable to believe that future legislation is incoming. Interestingly enough, the area of Deep Fakes has been a vibrant source of federal debate. There have been provisions that pertain to AI, for example, in 2019, the National Defense Authorization Act contained a provision regarding deep fakes (Web Page 2019). In particular, the act set up standards for the government to deal with the weaponization of deep fakes by foreign governments, and also established “Deepfake Prize competitions” to fund additional research (Web Page 2019). Both the House and Senate had their own form of Deep Fake legislation in 2019: the Identifying Outputs of Generative Adversarial Networks (IOGAN) Act in the House, and the Deepfake Report Act of 2019 in the Senate. Neither piece of legislation successfully passed the converse chamber, and was able to be signed by the president. In culmination, the status of deep fake legislation seems to be a recognition that deep fakes are a potential problem worthy of regulation on the federal level, but no major bill attempting to ban or limit them have been proposed. The strong, and vibrant discussion of AI ethics in the United States makes it quite likely that future regulation should eventually pass. In February 2019, the House of Representatives introduced H.R. 153 “Supporting the Development of Guidelines for Ethical Development of Artificial Intelligence,” which set up guidelines for accountability in automated decision making (Chae 2020). Later that year in April 2019, the Senate and House introduced the “Algorithmic Accountability Act” which would “impact assessments” on “automated decision systems in order to evaluate the impacts of the system’s design process and training data on “accuracy, fairness, bias, discrimination, privacy, and security” (Chae 2020). The same year, the Senate also introduced the “Commercial Facial Recognition Privacy Act” that would essentially have set up regulation covering the use of facial recognition technology, and introduced privacy rights for consumers in the case of this specific technology by banning their use by non-covered entities “without providing notice and obtaining their consent” from consumers (Chae 2020). Meanwhile, the White House, in 2019 released a memo titled “Guidance for Regulation of Artificial Intelligence Applications” (Vought 2020). Notably in 2019,the FTC fined Facebook $5 billion for “deceiving users about their ability to control the privacy of their personal information,” at the time more than twenty times larger than any other privacy enforcement fine levvied by the FTC (Press Release 2019). However, none of this information has resulted in significant federal legislation regulating AI systems. The information cumulatively suggests that while there is an appetite for AI regulation, the United States is not yet at a point where such regulation can be passed on a national level. Figure 11.1: FTC 2019 penalty for Facebook 11.3 California In 2018, California passed the California Consumer Privacy Act (CCPA). The legislation established a series of data privacy rights, all of which affect trained AI models, such as, the “right to know” what personal information a business collects on them, the “right to delete” personal information, the “right to opt-out” of sale of their personal information (Electronic Article, n.d.b). Penalties are quite severe, and start at $2,500 for each violation, and $7,500 for each “intentional subsequent violation” (Generic 2019). In addition, California Privacy Regulation Act (CPRA) passed in November 2020 builds on the CCPA and is scheduled to go into effect January 1, 2023 (Web Page 2020a). Essentially, this second bill creates a new agency in California, the “Creation of the California Privacy Protection Agency” (CPPA), an agency which is vested with enforcement of the CCPA and CPRA. With regards to new rights and regulations, this new piece of regulation establishes a new right to correction for consumers to correct incorrect data. Opt-in consent is required for collection of all personal information for children under the age of 16. The right to limit use of personal consumer information is also added. Businesses are faced with more severe data retention limits, and are required to disclose data retention periods. Consumers are also given additional rights to opt-out of the disclosure of personal information. The act also establishes a stronger right to private action for any citizens that experience “the unauthorized access and exfiltration, theft, or disclosure of an email address in combination with a password or security question and answer that could permit access to content.” All in all, the information suggests that while the federal government may not be providing a source of active legislation, the states are functioning as a laboratory for legislation. Thus, while no significant legislation currently exists, it is very reasonable to expect significant legislation to appear in the upcoming years. 11.4 Virginia As of March 2021, Virginia became the second state to pass comprehensive data privacy legislation; their legislation is modeled on California’s CCPA which is in turn modeled on the European Union’s GDPR (Web Page 2021b). This legislation titled the “Virginia Consumer Data Privacy Act” (VCDPA) is scheduled to go into effect on January 1, 2023. The bill applies to all entities who “who conduct business in the commonwealth of Virginia or produce products or services that are targeted to residents of the Commonwealth” and either “control or process personal data of at least 100,000 Virginia residents, or”derive over 50% of gross revenue from the sale of personal data (though the statute is unclear as to whether the revenue threshold applies to Virginia residents only) and control or process personal data of at least 25,000 Virginia residents\". Notably, the bill carves out a series of exemptions for healthcare institutions, non-profits, and institutes of higher education. With regards to data privacy, the bill establishes a series of rights in the vein of California’s CCPA, and CRPA. One major difference between the Virginia VCDPA and the CCPA that it “leav[es] enforcement entirely up to the Attorney General and [does not provide] even a private right of action for consumers.” Each violation can bring a fine of up to $7500 to be levied by the attorney general’s office (Web Page 2021b). The VCDPA also falls short in that it does not offer consumers “a private right to action” “for consumers whose nonencrypted and nonredacted personal information was subject to unauthorized access and exfiltration” (Web Page 2021b). With regards to rights established, several consumer rights are established. On the consumer side, consumers gain rights to “access, correction, deletion, data portability, and anti-discrimination” regarding their data (Web Page 2021b). Consumers also gain the “right to opt-out of sale of personal data, targeting advertising, and profiling.” Finally, consumers gain the right to consent for processing of “sensitive personal information” (essentially any sort of personally identifiable personal information, sexual orientation or immigration status), essentially forcing such data processing to become an “opt-in” decision for consumers. Businesses are required to engage in “data minimization and technical safeguard requirements,” and “data protection assessments and data processing agreements.” The former essentially requires companies to minimize the amount of data they collect, and establishes standards for companies to take reasonable steps to ensure safety of personal consumer data. The latter requires companies to lay out mechanisms and standards to process and delete data when consumers request so. 11.5 Illinois Illinois has not had any comprehensive data privacy or artificial intelligence regulation, rather choosing to engage in a “patchwork” approach with regards to data regulation. The Illinois General Assembly introduced the “Artificial Intelligence Video Interview Act” which established a rights for prospective employees whose video interviews would be evaluated by AI (Web Page 2021c). Said legislation required that employers notified interviewees if their video interviews would be evaluated by AI, and required employers to explain “how the artificial intelligence works and what general types of characteristics it uses to evaluate applicants” (Web Page 2021c). In addition, like, California, the Illinois House introduced and passed a privacy bill, titled the “2020 Illinois Data Transparency and Privacy Act” (Web Page 2020b). This law affects all “natural persons residing in Illinois,” and applies to businesses that “collects or discloses the personal information of 50,000 or more persons, Illinois households, or a combination thereof,” and businesses that “derive 50% or more of its annual revenues from selling consumer’s personal information.” It establishes a series of sweeping data privacy rights for consumers, such as the “right to know,” the “right to opt out,” the “right to correction,” and “right to deletion.” While the “right to correction” and “right to deletion” are fairly straightforward, the “right to know” establishes a right for consumers to request which companies have their information and the information that companies have on them. In addition, the “right to opt out” establishes a right to opt out of “disclosure [or sale] of personal information from a business to third parties,” and “the processing of personal information.” Unlike the California law, this bill did not successfully never received a vote in the Senate, and thus did not become law. Nontheless, the fact that such a bill was passed in one chamber of the state legislature strongly supports the notion that addition GDPR-style regulation will come for additional states besides California and Virginia. 11.6 Other States Roughly 30 states have introduced some sort of general consumer data privacy act, yet only two (Virginia and California) have managed to get those same bills passed or signed (Web Page 2021d). The remaining 28 states have had their state bills die in the legislative process. Just as with Illinois, state legislation has regularly been introduced. Figure 11.2: State data privacy regulation as of April 2021 One of the advantages of the United States is it’s Federalist system, wherein states operate like legislative laboratories for the Federal Government. It certainly appears that before additional federal legislation appears, additional state legislation will need to appear and be enacted. 11.7 Conclusion Regardless, it seems like a variety of rights are here to stay. Specifically, the right to correction, nondescrimination, mandatory encryption, and possibly the right to consent for processing of personal information. The right to opt-out of the sale of personal data, likewise with it’s appearance in Virginia and California and other GDPR inspired rights are here to stay. Even if the rights have yet to be codified,; it seems likely that data minimization for businesses are here to stay in the future. 11.8 Resources An article to read: “Garbage in, Garbage Out” by Claire Garvey https://www.flawedfacedata.com/# A story to read: [____] "],["13-future.html", "Chapter 12 Law: The future of regulation 12.1 Introduction 12.2 The Short Term 12.3 The Long Term View", " Chapter 12 Law: The future of regulation 12.1 Introduction As of now it’s quite hard to say what the future of AI/data regulation will be around the world. What is clear is that the world is in a stage of relative infancy with regards to the regulation that does exist. The future can be divided into both the short term view, and the long term view. Much of what is going to occur in the short term is likely inevitable, and can be largely predicted from what currently exists. The long term view will be much more speculative, but also simultaneously much richer in the possibilities for AI/data privacy regulation. 12.2 The Short Term AI regulation has a long way to go. It is in a period of infancy, and relative experimentation. The European Union, have passed laws are still figuring out how to optimize the enforcement mechanisms available to them. In the European Union, it is likely that enforcement will ramp up.The United States is relatively early on this process; there has been very little federal activity, and much experimentation on the state level. The federal government has made definitive steps to outline the rights to privacy of individuals, and structuring how those rights can fit into additional regulation. 12.3 The Long Term View In the long term, regulation is likely to become the norm. It is hard to believe that with the wealth of consumer privacy bills in the pipeline that said regulation is on its way out. For the countries that do not have legislation, any tech company that seeks to do international business will be affected by some sort of regulation, it would be nice to see companies continue to take a step forward in terms of leading the charge on regulation. Collectively, this information suggests that there is a strong appetite for additional regulation, and that more will be coming on the way. The industry is learning to adapt to the regulation in a variety of ways. Overall, companies are committed to having a place at the table of regulation. The future is still unclear, and time will only tell how the industry as a whole will respond. "],["14-references.html", "Chapter 13 References", " Chapter 13 References Agarwal, Shuhbam. 2020. “Are Deepfakes a Dangerous Technology? Creators and Regulators Disagree.” Journal Article. Digitaltrends. https://www.digitaltrends.com/features/deepfakes-future-of-internet/. Blog. 2015. https://www.kdnuggets.com/2015/11/poll-ethics-part-data-science-training.html. ———. 2016. https://about.fb.com/news/2016/07/facebook-diversity-update-positive-hiring-trends-show-progress/. Boyd, Danah. 2017. “Your Data Is Being Manipulated.” Journal Article. Points: Data &amp; Society. https://points.datasociety.net/your-data-is-being-manipulated-a7e31a83577b. Buolamwini, Joy, and Timnit Gebru. n.d. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” Conference Proceedings. In Conference on Fairness, Accountability and Transparency, 77–91. PMLR. http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf. Chae, Y. 2020. “US AI Regulation Guide: Legislative Overview and Practical Considerations. The Journal of Robotics.” Journal Article. Artificial Intelligence and Law 3 (1): 17–40. https://www.bakermckenzie.com/-/media/files/people/chae-yoon/rail-us-ai-regulation-guide.pdf. Chakravorti, Bhaskar. 2020. “To Increase Diversity, u.s. Tech Companies Need to Follow the Talent.” Journal Article. Harvard Business Review. https://hbr.org/2020/12/to-increase-diversity-u-s-tech-companies-need-to-follow-the-talent. Chivers, Tom. 2019. “What Do We Do about Deepfake Video?” Journal Article. The Guardian. https://www.theguardian.com/technology/2019/jun/23/what-do-we-do-about-deepfake-video-ai-facebook. Dickey, Megan Rose. 2021. “Examining the ‘Pipeline Problem’.” Journal Article. Techcrunch. https://techcrunch.com/2021/02/14/examining-the-pipeline-problem/. Electronic Article. n.d.a. https://sociology.unc.edu/undergraduate-program/sociology-major/what-is-sociology/. ———. n.d.b. https://oag.ca.gov/privacy/ccpa. Eubanks, Virginia. 2018. Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. Book. St. Martin’s Press. Film or Broadcast. 2018. https://www.youtube.com/watch?v=vUqC8UPw9SU&amp;t=2s. Generic. 2019. https://bakerlaw.com/webfiles/Privacy/2019/Briefs/California-Consumer-Privacy-Act-FAQs.pdf. Géron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Book. O’Reilly Media. Goodwin, Kiara. 2020. “Ethical Considerations of Deepfakes.” Journal Article. The Prindle Post. https://www.prindlepost.org/2020/12/ethical-considerations-of-deepfakes/. Hicks, Mar. 2020. “Built to Last.” Magazine Article. Logic, no. 11. https://logicmag.io/care/built-to-last/. Ho, Vanessa. 2018. “How Snow Leopard Selfies and AI Can Help Save the Species from Extinction.” Journal Article. Microsoft. https://news.microsoft.com/transform/snow-leopard-selfies-ai-save-species/. Hoffman, Kelly M, Sophie Trawalter, Jordan R Axt, and M Norman Oliver. 2016. “Racial Bias in Pain Assessment and Treatment Recommendations, and False Beliefs about Biological Differences Between Blacks and Whites.” Journal Article. Proceedings of the National Academy of Sciences 113 (16): 4296–4301. Hutson, Matthew. 2021. “Who Should Stop Unethical AI?” Journal Article. The New Yorker. https://www.newyorker.com/tech/annals-of-technology/who-should-stop-unethical-ai. Julia Angwin, Surya Mattu, Jeff Larson, and Lauren Kirchner. 2016. “Machine Bias.” Journal Article. ProPublica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing. Kelion, Leo. 2020. “Deepfake Detection Tool Unveiled by Microsoft.” Journal Article. BBC. https://www.bbc.com/news/technology-53984114. Kelly, Makena. 2020. “UNEMPLOYMENT CHECKS ARE BEING HELD UP BY a CODING LANGUAGE ALMOST NOBODY KNOWS.” Journal Article. The Verge. https://www.theverge.com/2020/4/14/21219561/coronavirus-pandemic-unemployment-systems-cobol-legacy-software-infrastructure. Kerry, Cameron. 2020. “Protecting Privacy in an AI-Driven World.” Journal Article. Https://Www. Brookings. Edu/Research/Protecting-Privacy-in-an-Ai-Driven-World/. Lacoste, Alexandre, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. “Quantifying the Carbon Emissions of Machine Learning.” Journal Article. arXiv Preprint arXiv:1910.09700. https://arxiv.org/pdf/1910.09700.pdf. Latonero, Mark. 2019. “Opinion: AI for Good Is Often Bad.” Journal Article. https://www.wired.com/story/opinion-ai-for-good-is-often-bad/. Li, He, Lu Yu, and Wu He. 2019. “The Impact of GDPR on Global Technology Development.” Journal Article. Journal of Global Information Technology Management 22 (1). https://www.tandfonline.com/doi/pdf/10.1080/1097198X.2019.1569186. Lomas, Natasha. 2021. “Grindr on the Hook for €10m over GDPR Consent Violations.” Journal Article. Techcrunch. https://techcrunch.com/2021/01/26/grindr-on-the-hook-for-e10m-over-gdpr-consent-violations/. Meghani, Salimah H, Eeeseung Byun, and Rollin M Gallagher. 2012. “Time to Take Stock: A Meta-Analysis and Systematic Review of Analgesic Treatment Disparities for Pain in the United States.” Pain Medicine 13 (2): 150–74. Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2019. “A Survey on Bias and Fairness in Machine Learning.” Journal Article. arXiv Preprint arXiv:1908.09635. Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. n.d. “Model Cards for Model Reporting.” Conference Proceedings. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 220–29. https://arxiv.org/pdf/1810.03993.pdf?source=post_page---------------------------. Morozov, Evgeny. 2013. “THE FOLLY OF TECHNOLOGICAL SOLUTIONISM: AN INTERVIEW WITH EVGENY MOROZOV.” Magazine Article. Public Books. https://www.publicbooks.org/the-folly-of-technological-solutionism-an-interview-with-evgeny-morozov/. Newspaper Article. 2018. https://www.nytimes.com/2018/05/24/technology/europe-gdpr-privacy.html. ———. 2020. https://www.nytimes.com/2020/04/27/technology/GDPR-privacy-law-europe.html. Nick Palmieri, Baker Botts. 2021. “GDPR Enforcement Was up in 2020. What Does That Mean for You in 2021?” Journal Article. Law.com. https://www.law.com/legaltechnews/2021/02/17/gdpr-enforcement-was-up-in-2020-what-does-that-mean-for-you-in-2021/?slreturn=20210228201338. Phillips, Whitney. 2018. “The Oxygen of Amplification.” Journal Article. Data &amp; Society 22: 1–128. Press Release. 2019. https://www.ftc.gov/news-events/press-releases/2019/07/ftc-imposes-5-billion-penalty-sweeping-new-privacy-restrictions. Rudin, Cynthia. 2018. “Algorithms and Justice: Scrapping the ‘Black Box’.” Journal Article. The Crime Report. https://thecrimereport.org/2018/01/26/algorithms-and-justice-scrapping-the-black-box/. Schwartz, Roy, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2019. “Green Ai.” Journal Article. arXiv Preprint arXiv:1907.10597. https://arxiv.org/pdf/1907.10597.pdf. Strickland, Eliza. 2019. “Facebook AI Launches Its Deepfake Detection Challenge.” Journal Article. IEEE Spectrum. https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/facebook-ai-launches-its-deepfake-detection-challenge. Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy and Policy Considerations for Deep Learning in NLP.” Journal Article. arXiv Preprint arXiv:1906.02243. https://arxiv.org/pdf/1906.02243.pdf. Torralba, Antonio, and Alexei A Efros. 2011. “Unbiased Look at Dataset Bias.” In CVPR 2011, 1521–28. IEEE. Vosoughi, Soroush, Deb Roy, and Sinan Aral. 2018. “The Spread of True and False News Online.” Journal Article. Science 359 (6380): 1146–51. Vought, Russell T. 2020. “Re: Guidance for Regulation of Artificial Intelligence Applications.” Journal Article. https://www.whitehouse.gov/wp-content/uploads/2020/01/Draft-OMB-Memo-on-Regulation-of-AI-1-7-19.pdf. Web Page. 2008. https://snowleopardconservancy.org/kids/text/endangered.htm. ———. 2019. https://www.jdsupra.com/legalnews/first-federal-legislation-on-deepfakes-42346/. ———. 2020a. https://www.jdsupra.com/legalnews/california-s-new-privacy-law-the-crpa-93354/. ———. 2020b. https://www.bytebacklaw.com/2020/01/analyzing-the-2020-illinois-data-transparency-and-privacy-act/. ———. 2021b. https://www.gibsondunn.com/virginia-passes-comprehensive-privacy-law/. ———. 2021a. https://www.enforcementtracker.com/. ———. 2021d. https://iapp.org/resources/article/state-comparison-table/. ———. 2021c. https://www.ncsl.org/research/telecommunications-and-information-technology/2020-legislation-related-to-artificial-intelligence.aspx. "]]
