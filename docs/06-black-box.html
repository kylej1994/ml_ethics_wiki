<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Ethics: Black Box Models | ML Ethics Wiki</title>
  <meta name="description" content="Chapter 5 Ethics: Black Box Models | ML Ethics Wiki" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Ethics: Black Box Models | ML Ethics Wiki" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Ethics: Black Box Models | ML Ethics Wiki" />
  
  
  

<meta name="author" content="Kyle Jablon" />


<meta name="date" content="2021-04-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="05-environment.html"/>
<link rel="next" href="07-amplification.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MLEthics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="02-intro.html"><a href="02-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="02-intro.html"><a href="02-intro.html#the-problem"><i class="fa fa-check"></i><b>1.1</b> The Problem</a></li>
<li class="chapter" data-level="1.2" data-path="02-intro.html"><a href="02-intro.html#who-is-this-for"><i class="fa fa-check"></i><b>1.2</b> Who is this for?</a></li>
<li class="chapter" data-level="1.3" data-path="02-intro.html"><a href="02-intro.html#why-do-ethics-matter"><i class="fa fa-check"></i><b>1.3</b> Why do ethics matter?</a></li>
<li class="chapter" data-level="1.4" data-path="02-intro.html"><a href="02-intro.html#why-does-sociology-matter"><i class="fa fa-check"></i><b>1.4</b> Why does sociology matter?</a></li>
<li class="chapter" data-level="1.5" data-path="02-intro.html"><a href="02-intro.html#why-does-institutionalization-matter"><i class="fa fa-check"></i><b>1.5</b> Why does institutionalization matter?</a></li>
<li class="chapter" data-level="1.6" data-path="02-intro.html"><a href="02-intro.html#what-is-machine-learning-and-why-is-it-relevant"><i class="fa fa-check"></i><b>1.6</b> What is machine learning and why is it relevant?</a></li>
<li class="chapter" data-level="1.7" data-path="02-intro.html"><a href="02-intro.html#additional-resources"><i class="fa fa-check"></i><b>1.7</b> Additional Resources</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="03-dataset-bias.html"><a href="03-dataset-bias.html"><i class="fa fa-check"></i><b>2</b> Ethics: Dataset bias and it’s consequences</a>
<ul>
<li class="chapter" data-level="2.1" data-path="03-dataset-bias.html"><a href="03-dataset-bias.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="03-dataset-bias.html"><a href="03-dataset-bias.html#the-problem-1"><i class="fa fa-check"></i><b>2.2</b> The problem</a></li>
<li class="chapter" data-level="2.3" data-path="03-dataset-bias.html"><a href="03-dataset-bias.html#how-to-identify-the-problem"><i class="fa fa-check"></i><b>2.3</b> How to identify the problem?</a></li>
<li class="chapter" data-level="2.4" data-path="03-dataset-bias.html"><a href="03-dataset-bias.html#resources"><i class="fa fa-check"></i><b>2.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="04-privacy-rights.html"><a href="04-privacy-rights.html"><i class="fa fa-check"></i><b>3</b> Ethics: Privacy Rights</a>
<ul>
<li class="chapter" data-level="3.1" data-path="04-privacy-rights.html"><a href="04-privacy-rights.html#what-do-privacy-rights-have-to-do-with-tech"><i class="fa fa-check"></i><b>3.1</b> What do Privacy Rights have to do with tech?</a></li>
<li class="chapter" data-level="3.2" data-path="04-privacy-rights.html"><a href="04-privacy-rights.html#what-are-privacy-rights"><i class="fa fa-check"></i><b>3.2</b> What are privacy rights?</a></li>
<li class="chapter" data-level="3.3" data-path="04-privacy-rights.html"><a href="04-privacy-rights.html#rules-of-thumb"><i class="fa fa-check"></i><b>3.3</b> Rules of thumb</a></li>
<li class="chapter" data-level="3.4" data-path="04-privacy-rights.html"><a href="04-privacy-rights.html#resources-1"><i class="fa fa-check"></i><b>3.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="05-environment.html"><a href="05-environment.html"><i class="fa fa-check"></i><b>4</b> Ethics: Environmental Costs of Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="05-environment.html"><a href="05-environment.html#the-problem-2"><i class="fa fa-check"></i><b>4.1</b> The Problem</a></li>
<li class="chapter" data-level="4.2" data-path="05-environment.html"><a href="05-environment.html#how-to-identify-the-problem-1"><i class="fa fa-check"></i><b>4.2</b> How to identify the problem</a></li>
<li class="chapter" data-level="4.3" data-path="05-environment.html"><a href="05-environment.html#what-can-be-done"><i class="fa fa-check"></i><b>4.3</b> What can be done</a></li>
<li class="chapter" data-level="4.4" data-path="05-environment.html"><a href="05-environment.html#resources-2"><i class="fa fa-check"></i><b>4.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="06-black-box.html"><a href="06-black-box.html"><i class="fa fa-check"></i><b>5</b> Ethics: Black Box Models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="06-black-box.html"><a href="06-black-box.html#the-problem-3"><i class="fa fa-check"></i><b>5.1</b> The Problem</a></li>
<li class="chapter" data-level="5.2" data-path="06-black-box.html"><a href="06-black-box.html#which-models-are-black-box-models"><i class="fa fa-check"></i><b>5.2</b> Which models are black box models</a></li>
<li class="chapter" data-level="5.3" data-path="06-black-box.html"><a href="06-black-box.html#what-can-be-done-1"><i class="fa fa-check"></i><b>5.3</b> What can be done</a></li>
<li class="chapter" data-level="5.4" data-path="06-black-box.html"><a href="06-black-box.html#resources-3"><i class="fa fa-check"></i><b>5.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="07-amplification.html"><a href="07-amplification.html"><i class="fa fa-check"></i><b>6</b> Ethics: Amplification</a>
<ul>
<li class="chapter" data-level="6.1" data-path="07-amplification.html"><a href="07-amplification.html#the-problem-4"><i class="fa fa-check"></i><b>6.1</b> The Problem</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="07-amplification.html"><a href="07-amplification.html#algorithmic-determinism"><i class="fa fa-check"></i><b>6.1.1</b> Algorithmic Determinism</a></li>
<li class="chapter" data-level="6.1.2" data-path="07-amplification.html"><a href="07-amplification.html#amplifying-misinformation"><i class="fa fa-check"></i><b>6.1.2</b> Amplifying Misinformation</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="07-amplification.html"><a href="07-amplification.html#what-can-be-done-2"><i class="fa fa-check"></i><b>6.2</b> What can be done</a></li>
<li class="chapter" data-level="6.3" data-path="07-amplification.html"><a href="07-amplification.html#resources-4"><i class="fa fa-check"></i><b>6.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="08-deep-fakes.html"><a href="08-deep-fakes.html"><i class="fa fa-check"></i><b>7</b> Ethics: Deep Fakes</a>
<ul>
<li class="chapter" data-level="7.1" data-path="08-deep-fakes.html"><a href="08-deep-fakes.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="08-deep-fakes.html"><a href="08-deep-fakes.html#what-can-be-done-3"><i class="fa fa-check"></i><b>7.2</b> What can be done</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="09-solutionism.html"><a href="09-solutionism.html"><i class="fa fa-check"></i><b>8</b> Sociology: Technological Solutionism</a>
<ul>
<li class="chapter" data-level="8.1" data-path="09-solutionism.html"><a href="09-solutionism.html#what-is-technological-solutionism"><i class="fa fa-check"></i><b>8.1</b> What is technological solutionism?</a></li>
<li class="chapter" data-level="8.2" data-path="09-solutionism.html"><a href="09-solutionism.html#the-problem-5"><i class="fa fa-check"></i><b>8.2</b> The problem</a></li>
<li class="chapter" data-level="8.3" data-path="09-solutionism.html"><a href="09-solutionism.html#what-can-be-done-4"><i class="fa fa-check"></i><b>8.3</b> What can be done?</a></li>
<li class="chapter" data-level="8.4" data-path="09-solutionism.html"><a href="09-solutionism.html#resources-5"><i class="fa fa-check"></i><b>8.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="10-myths.html"><a href="10-myths.html"><i class="fa fa-check"></i><b>9</b> Sociology: Myths about Silicon Valley and its Workers</a>
<ul>
<li class="chapter" data-level="9.1" data-path="10-myths.html"><a href="10-myths.html#introduction-3"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="10-myths.html"><a href="10-myths.html#diversity-in-tech-is-a-pipeline-problem"><i class="fa fa-check"></i><b>9.2</b> Diversity in Tech is a “pipeline problem”</a></li>
<li class="chapter" data-level="9.3" data-path="10-myths.html"><a href="10-myths.html#systems-need-to-be-maintained"><i class="fa fa-check"></i><b>9.3</b> Systems need to be maintained</a></li>
<li class="chapter" data-level="9.4" data-path="10-myths.html"><a href="10-myths.html#resources-6"><i class="fa fa-check"></i><b>9.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="11-gdpr.html"><a href="11-gdpr.html"><i class="fa fa-check"></i><b>10</b> Law: GDPR</a>
<ul>
<li class="chapter" data-level="10.1" data-path="11-gdpr.html"><a href="11-gdpr.html#what-is-gdpr"><i class="fa fa-check"></i><b>10.1</b> What is GDPR?</a></li>
<li class="chapter" data-level="10.2" data-path="11-gdpr.html"><a href="11-gdpr.html#what-are-the-current-problems-with-gdpr"><i class="fa fa-check"></i><b>10.2</b> What are the current problems with GDPR?</a></li>
<li class="chapter" data-level="10.3" data-path="11-gdpr.html"><a href="11-gdpr.html#how-is-gdpr-relevant-to-regulation-around-the-world"><i class="fa fa-check"></i><b>10.3</b> How is GDPR relevant to regulation around the world?</a></li>
<li class="chapter" data-level="10.4" data-path="11-gdpr.html"><a href="11-gdpr.html#resources-7"><i class="fa fa-check"></i><b>10.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="12-us.html"><a href="12-us.html"><i class="fa fa-check"></i><b>11</b> Law: Regulation in the United States</a>
<ul>
<li class="chapter" data-level="11.1" data-path="12-us.html"><a href="12-us.html#introduction-4"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="12-us.html"><a href="12-us.html#the-national-discussion"><i class="fa fa-check"></i><b>11.2</b> The National Discussion</a></li>
<li class="chapter" data-level="11.3" data-path="12-us.html"><a href="12-us.html#california"><i class="fa fa-check"></i><b>11.3</b> California</a></li>
<li class="chapter" data-level="11.4" data-path="12-us.html"><a href="12-us.html#virginia"><i class="fa fa-check"></i><b>11.4</b> Virginia</a></li>
<li class="chapter" data-level="11.5" data-path="12-us.html"><a href="12-us.html#illinois"><i class="fa fa-check"></i><b>11.5</b> Illinois</a></li>
<li class="chapter" data-level="11.6" data-path="12-us.html"><a href="12-us.html#other-states"><i class="fa fa-check"></i><b>11.6</b> Other States</a></li>
<li class="chapter" data-level="11.7" data-path="12-us.html"><a href="12-us.html#conclusion"><i class="fa fa-check"></i><b>11.7</b> Conclusion</a></li>
<li class="chapter" data-level="11.8" data-path="12-us.html"><a href="12-us.html#resources-8"><i class="fa fa-check"></i><b>11.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="13-future.html"><a href="13-future.html"><i class="fa fa-check"></i><b>12</b> Law: The future of regulation</a>
<ul>
<li class="chapter" data-level="12.1" data-path="13-future.html"><a href="13-future.html#introduction-5"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="13-future.html"><a href="13-future.html#the-short-term"><i class="fa fa-check"></i><b>12.2</b> The Short Term</a></li>
<li class="chapter" data-level="12.3" data-path="13-future.html"><a href="13-future.html#the-long-term-view"><i class="fa fa-check"></i><b>12.3</b> The Long Term View</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="14-references.html"><a href="14-references.html"><i class="fa fa-check"></i><b>13</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ML Ethics Wiki</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ethics-black-box-models" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Ethics: Black Box Models</h1>
<div id="the-problem-3" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> The Problem</h2>
<p>From Philip K Dick’s science fiction classic “Minority Report” describing the notion of “precrime”:</p>
<pre><code>“You’re acquainted with the theory of precrime, of course. I presume we can take that for granted.”

 “I have the information publicly available,” Witwer replied. “With the aid of your precog mutants, you’ve
boldly and successfully abolished the post-crime punitive system of jails and fines. 

[...] 

Anderton said: “You’ve probably already grasped the basic legalistic drawback to precrime methodology. We’re taking in individuals who have broken no law.”

 “But surely, they will,” Witwer affirmed with conviction.
 
 “Happily, they don’t — because we get to them first, before they can commit an act of violence. So the
commission of the crime itself is absolute metaphysics. We can claim they are culpable. They, on the other
hand, can eternally claim they’re innocent. And, in a sense, they are innocent.”</code></pre>
<p>The notion of “precrime” isn’t merely science fiction. In Philip K. Dick’s story, “precog” mutants who can see the future are used to predict future crimes and then charge and jail prospective offenders, despite the fact that they have not committed any crimes. In the real world, there are parallels that already exist. The COMPAS system is used to predict rates of re offending by prospective criminals, and adjust their sentences accordingly. More importantly, this software is proprietary, and uses up to 130 factors, “including criminal history, age, gender and other information, such as whether their mother was ever arrested or whether they have trouble paying bills” <span class="citation">(<a href="14-references.html#ref-rudin2018" role="doc-biblioref">Rudin 2018</a>)</span>. COMPAS has made mistakes in the past, for example, in 2016, an inmate was mistakenly denied parole because an employee mistakenly filled an incorrect answer on a form <span class="citation">(<a href="14-references.html#ref-rudin2018" role="doc-biblioref">Rudin 2018</a>)</span>. These risk scores are used for sentencing requirements, and have been shown to disproportionately rate black candidates as being “higher risk” <span class="citation">(<a href="14-references.html#ref-angwin2016" role="doc-biblioref">Julia Angwin and Kirchner 2016</a>)</span>.</p>
<p>These are all examples of “black box models.” Black box models refer primarily to those models relationship between inputs and generated outputs are not human interpretable. Black box models do not merely refer to machine learning models, although that is primarily where they come up; a financial model can be a black box model if the relationship between inputs and outputs is inscrutable to common users. In other words, a model such as COMPAS is a black box model, in so far as its logic and mechanisms are a invisible to the user, and cannot be deciphered. There is merely the data, the model, and an output which is spit out. Black box models have particular issues in the sense that they can easily exacerbate existing issues of inequality, and make it difficult to carve out exceptions to cases that ought not be treated in a simple way.</p>
<p>In particular, black box models can be particularly problematic in exacerbating underlying racial or socioeconomic relationships in data. For example, a black box model that assigns insurance premiums can “learn” notions of race from the underlying characteristics of data, such as perhaps zip code or other socioeconomic information and then amplify existing underlying biases. The nature of these models as black boxes means that they can essentially automate pre-existing inequality with little to no accountability for the problematic nature of the model.</p>
<!--- %\subsubsection{Blackbox Models and Race}
% [talk about how it is really really bad in some fields]


% [Graphic] Investopedia: https://images.app.goo.gl/9AvnT2qph5YDwx877 --->
</div>
<div id="which-models-are-black-box-models" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Which models are black box models</h2>
<p>Models can become “black boxes” for a few primary reasons: typically this can be because a model is proprietary, user impenetrable, or their structure is quite literally not human scrutable. For example, COMPAS, is a proprietary model, an interpretable model that does not explain its predictions to its users is still a black box, and finally a model where no one understands how it works would not be human scrutable.</p>
</div>
<div id="what-can-be-done-1" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> What can be done</h2>
<p>It should be noted that there are black box models, and black box systems. Not every model necessarily needs to be an open book. Google Translate is an incredibly complicated neural network model, and while it would be nice for it to be human scrutable, but it is likely unnecessary for this to be totally human interpretable. Meanwhile, an algorithm that prices insurance rates for different people should be explainable, and a black box system creates an opportunity for discrimination. This distinction is important, because merely constructing an explainable model does not necessarily lead to explainable systems. In other words, having an interpretable model is a necessary but insufficient standard for an interpretable machine learning system.</p>
<p>The best solution of all is a simple model, and regression models are typically the most straight forward of all. If a simple model can get the job done, it goes without saying that that should always be the first option. Regression models have extensive explainability tools built around them, such as LIME or SHAP, and are also easiest to build more tooling on top of, due to their widespread use and availability.</p>
<p>If the typical regression models (OLS and logistic) cannot do the trick it has been suggested that the most powerful and explainable model possible is an xgboost model when used in conjunction with shapley <span class="citation">(<a href="14-references.html#ref-hall2018" role="doc-biblioref">Film or Broadcast 2018</a>)</span>. In particular, the ability to manually set monotonicity of the input features results in incredibly interpretable models. For example, if one is predicting health insurance premiums, age can be monotonically related to one’s premiums and thus as age increases then one’s premiums will naturally go up. In addition to the SHAP framework, which is a game theoretic approach to explainable ML, this is an incredibly powerful tool.</p>
<p>Another mechanism if a model is particularly complicated, then a user can train a complicated model and then train an explainable model to predict the output of the complicated model. This solution typically results in a moderate degrade in accuracy, but is nonetheless a very popular workaround to convert a black box model to an explainable one.</p>
<p><span class="citation"><a href="14-references.html#ref-mitchell2019" role="doc-biblioref">Mitchell et al.</a> (<a href="14-references.html#ref-mitchell2019" role="doc-biblioref">n.d.</a>)</span> has suggested standardizing model cards for educating cross-functional stakeholders on machine learning. Akin to a nutritional scorecard, but for a machine learning model, this is a model intended to standardize information about a machine learning model on a one-pager. This is an important step in standardizing explainable modeling. A model card would include information on training data, and the level of explainability provided by the model and system as a whole, in addition to any possible gaps in its coverage and possible biases. The mere act of standardizing these at an organization can go a long way towards norm creation in the indsutry as a whole.</p>
</div>
<div id="resources-3" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Resources</h2>
<p><strong>Interpretable ML Book by Christoph Molnar: </strong> <a href="https://christophm.github.io/interpretable-ml-book/" class="uri">https://christophm.github.io/interpretable-ml-book/</a></p>
<p><strong>The Minority Report by Philip K. Dick: </strong> <a href="https://cwanderson.org/wp-content/uploads/2011/11/Philip-K-Dick-The-Minority-Report.pdf" class="uri">https://cwanderson.org/wp-content/uploads/2011/11/Philip-K-Dick-The-Minority-Report.pdf</a></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="05-environment.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="07-amplification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-black-box.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/kylej1994/ml_ethics_wiki/blob/master/06-black-box.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
